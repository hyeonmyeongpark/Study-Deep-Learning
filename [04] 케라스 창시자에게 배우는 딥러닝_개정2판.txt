
* 메모장에서 바탕체 글꼴로 작성된 문서입니다

#########################################################
#
# 케라스 창시자에게 배우는 딥러닝 개정 2판
#
#########################################################

# 주교재

  - http://www.yes24.com/Product/Goods/112012471
  - https://thebook.io/080315/
  - https://livebook.manning.com/book/deep-learning-with-python-second-edition
  - https://github.com/rickiepark/deep-learning-with-python-2nd

  - (저자) François Chollet 
    ~ https://fchollet.com/
    ~ https://twitter.com/fchollet
    ~ https://github.com/fchollet

========================================
 0장 딥러닝이란 무엇인가?
========================================

  - 딥러닝 CNN.txt
  - 딥러닝 RNN.txt

  - Keras 이해와 활용.txt
  - 모두의 딥러닝 3판.txt


========================================
 1장 딥러닝이란 무엇인가?
========================================

  1.1 인공 지능과 머신 러닝, 딥러닝

    1.1.1 인공 지능

      - Symbolism VS Connectionism : https://nongnongai.tistory.com/3
             
    1.1.2 머신 러닝

      - 튜링 테스트 : https://news.einfomax.co.kr/news/articleView.html?idxno=4007639

    1.1.3 데이터에서 표현을 학습하기
    1.1.4 딥러닝에서 '딥'이란 무엇일까?
    1.1.5 그림 3개로 딥러닝의 작동 원리 이해하기
    1.1.6 지금까지 딥러닝의 성과
    1.1.7 단기간의 과대 선전을 믿지 말자
    1.1.8 AI에 대한 전망

  1.2 딥러닝 이전: 머신 러닝의 간략한 역사

    1.2.1 확률적 모델링
    1.2.2 초창기 신경망
    1.2.3 커널 방법

      - SVM

    1.2.4 결정 트리, 랜덤 포레스트, 그레이디언트 부스팅 머신
    1.2.5 다시 신경망으로
    1.2.6 딥러닝의 특징
    1.2.7 머신 러닝의 최근 동향

  1.3 왜 딥러닝일까? 왜 지금일까?

    1.3.1 하드웨어
    1.3.2 데이터
    1.3.3 알고리즘
    1.3.4 새로운 투자의 바람
    1.3.5 딥러닝의 대중화
    1.3.6 지속될까?


========================================
 2장 신경망의 수학적 구성 요소
========================================

  2.1 신경망과의 첫 만남

  2.2 신경망을 위한 데이터 표현

    2.2.1 스칼라(랭크- 0 텐서)
    2.2.2 벡터(랭크-1 텐서)
    2.2.3 행렬(랭크-2 텐서)
    2.2.4 랭크-3 텐서와 더 높은 랭크의 텐서
    2.2.5 핵심 속성
    2.2.6 넘파이로 텐서 조작하기
    2.2.7 배치 데이터
    2.2.8 텐서의 실제 사례
    2.2.9 벡터 데이터
    2.2.10 시계열 데이터 또는 시퀀스 데이터
    2.2.11 이미지 데이터
    2.2.12 비디오 데이터

  2.3 신경망의 톱니바퀴: 텐서 연산

    2.3.1 원소별 연산
    2.3.2 브로드캐스팅
    2.3.3 텐서 곱셈
    2.3.4 텐서 크기 변환
    2.3.5 텐서 연산의 기하학적 해석
    2.3.6 딥러닝의 기하학적 해석

      - 매니폴더 가설 : https://deepai.org/machine-learning-glossary-and-terms/manifold-hypothesis
      - Dimesion Reduction : PCA, 주성분분석 (1) : https://excelsior-cjh.tistory.com/167
      - Dimesion Reduction : LLE (2)             : https://excelsior-cjh.tistory.com/168

  2.4 신경망의 엔진: 그레이디언트 기반 최적화

    2.4.1 도함수란?
    2.4.2 텐서 연산의 도함수: 그레이디언트
    2.4.3 확률적 경사 하강법
    2.4.4 도함수 연결: 역전파 알고리즘

  2.5 첫 번째 예제 다시 살펴보기

    2.5.1 텐서플로를 사용하여 첫 번째 예제를 밑바닥부터 다시 구현하기

      - __call__ 함수 : https://wjunsea.tistory.com/61

    2.5.2 훈련 스텝 실행하기
    2.5.3 전체 훈련 루프
    2.5.4 모델 평가하기

  2.6 요약


========================================
 3장 케라스와 텐서플로 소개
========================================

  3.1 텐서플로란?

  3.2 케라스란?

  3.3 케라스와 텐서플로의 간략한 역사

  3.4 딥러닝 작업 환경 설정하기

    3.4.1 주피터 노트북: 권장하는 딥러닝 실험 도구
    3.4.2 코랩 사용하기

  3.5 텐서플로 시작하기

    3.5.1 상수 텐서와 변수
    3.5.2 텐서 연산: 텐서플로에서 수학 계산하기
    3.5.3 GradientTape API 다시 살펴보기
    3.5.4 엔드-투-엔드 예제: 텐서플로 선형 분류기

  3.6 신경망의 구조: 핵심 Keras API 이해하기

    3.6.1 층: 딥러닝의 구성 요소
    3.6.2 층에서 모델로
    3.6.3 '컴파일' 단계: 학습 과정 설정
    3.6.4 손실 함수 선택하기
    3.6.5 fit( ) 메서드 이해하기
    3.6.6 검증 데이터에서 손실과 측정 지표 모니터링하기
    3.6.7 추론: 훈련한 모델 사용하기

  3.7 요약


========================================
 4장 신경망 시작하기: 분류와 회귀
========================================

  4.1 영화 리뷰 분류: 이진 분류 문제

    4.1.1 IMDB 데이터셋
    4.1.2 데이터 준비
    4.1.3 신경망 모델 만들기
    4.1.4 훈련 검증
    4.1.5 훈련된 모델로 새로운 데이터에 대해 예측하기
    4.1.6 추가 실험
    4.1.7 정리


    # Training, Validation and Test sets 차이 및 정확한 용도
 
    - https://modern-manual.tistory.com/19
    - https://www.v7labs.com/blog/train-validation-test-set


  4.2 뉴스 기사 분류: 다중 분류 문제

    4.2.1 로이터 데이터셋
    4.2.2 데이터 준비
    4.2.3 모델 구성
    4.2.4 훈련 검증
    4.2.5 새로운 데이터에 대해 예측하기
    4.2.6 레이블과 손실을 다루는 다른 방법
    4.2.7 충분히 큰 중간층을 두어야 하는 이유
    4.2.8 추가 실험
    4.2.9 정리7

  4.3 주택 가격 예측: 회귀 문제

    4.3.1 보스턴 주택 가격 데이터셋
    4.3.2 데이터 준비
    4.3.3 모델 구성
    4.3.4 K-겹 검증을 사용한 훈련 검증
    4.3.5 새로운 데이터에 대해 예측하기
    4.3.6 정리

  4.4 요약


========================================
 5장 머신 러닝의 기본 요소
========================================

  5.1 일반화: 머신 러닝의 목표

    5.1.1 과소적합과 과대적합

    - random variable, sample space : https://www.mathsisfun.com/data/random-variables.html
    - (input) feature space     : https://towardsdatascience.com/concept-learning-and-feature-spaces-45cee19e49db

    5.1.2 딥러닝에서 일반화의 본질

  5.2 머신 러닝 모델 평가

    5.2.1 훈련, 검증, 테스트 세트
    5.2.2 상식 수준의 기준점 넘기
    5.2.3 모델 평가에 대해 유념해야 할 점

  5.3 훈련 성능 향상하기

    5.3.1 경사 하강법의 핵심 파라미터 튜닝하기
    5.3.2 구조에 대해 더 나은 가정하기
    5.3.3 모델 용량 늘리기

  5.4 일반화 성능 향상하기

    5.4.1 데이터셋 큐레이션
    5.4.2 특성 공학
    5.4.3 조기 종료 사용하기
    5.4.4 모델 규제하기

      - Regularization

  5.5 요약


========================================
 6장 일반적인 머신 러닝 워크플로
========================================

  6.1 작업 정의

    6.1.1 문제 정의
    6.1.2 데이터 수집
    6.1.3 데이터 이해
    6.1.4 성공 지표 선택

  6.2 모델 개발

    6.2.1 데이터 준비
    6.2.2 평가 방법 선택
    6.2.3 기준 모델 뛰어넘기
    6.2.4 모델 용량 키우기: 과대적합 모델 만들기
    6.2.5 모델 규제와 하이퍼파라미터 튜닝

  6.3 모델 배포

    6.3.1 고객에게 작업을 설명하고 기대치 설정하기
    6.3.2 추론 모델 배치하기
    6.3.3 작동 중 모델 모니터링하기
    6.3.4 모델 유지 관리

  6.4 요약


========================================
 7장 케라스 완전 정복
========================================

  7.1 다양한 워크플로

  7.2 케라스 모델을 만드는 여러 방법

    7.2.1 Sequential 모델
    7.2.2 함수형 API
    7.2.3 Model 서브클래싱
    7.2.4 여러 방식을 혼합하여 사용하기
    7.2.5 작업에 적합한 도구 사용하기

  7.3 내장된 훈련 루프와 평가 루프 사용하기

    7.3.1 사용자 정의 지표 만들기
    7.3.2 콜백 사용하기
    7.3.3 사용자 정의 콜백 만들기
    7.3.4 텐서보드를 사용한 모니터링과 시각화

  7.4 사용자 정의 훈련, 평가 루프 만들기

    7.4.1 훈련 vs 추론
    7.4.2 측정 지표의 저수준 사용법
    7.4.3 완전한 훈련과 평가 루프
    7.4.4 tf.function으로 성능 높이기
    7.4.5 fit( ) 메서드를 사용자 정의 루프로 활용하기

  7.5 요약


========================================
 8장 컴퓨터 비전을 위한 딥러닝
========================================

  8.1 합성곱 신경망 소개

    8.1.1 합성곱 연산
    8.1.2 최대 풀링 연산

  8.2 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

    8.2.1 작은 데이터셋 문제에서 딥러닝의 타당성
    8.2.2 데이터 내려받기
    8.2.3 모델 만들기
    8.2.4 데이터 전처리
    8.2.5 데이터 증식 사용하기

  8.3 사전 훈련된 모델 활용하기

    8.3.1 사전 훈련된 모델을 사용한 특성 추출
    8.3.2 사전 훈련된 모델 미세 조정하기

  8.4 요약


==============================================
 9장 컴퓨터 비전을 위한 고급 딥러닝
==============================================

  9.1 세 가지 주요 컴퓨터 비전 작업

  9.2 이미지 분할 예제

  9.3 최신 컨브넷 아키텍처 패턴

    9.3.1 모듈화, 계층화 그리고 재사용
    9.3.2 잔차 연결
    9.3.3 배치 정규화
    9.3.4 깊이별 분리 합성곱
    9.3.5 Xception 유사 모델에 모두 적용하기

  9.4 컨브넷이 학습한 것 해석하기

    9.4.1 중간 활성화 시각화
    9.4.2 컨브넷 필터 시각화하기
    9.4.3 클래스 활성화의 히트맵 시각화하기

  9.5 요약


========================================
 10장 시계열을 위한 딥러닝
========================================

  10.1 다양한 종류의 시계열 작업

  10.2 온도 예측 문제

    10.2.1 데이터 준비
    10.2.2 상식 수준의 기준점
    10.2.3 기본적인 머신 러닝 모델 시도해 보기
    10.2.4 1D 합성곱 모델 시도해 보기
    10.2.5 첫 번째 순환 신경망

  10.3 순환 신경망 이해하기

    10.3.1 케라스의 순환 층

  10.4 순환 신경망의 고급 사용법

    10.4.1 과대적합을 감소하기 위해 순환 드롭아웃 사용하기
    10.4.2 스태킹 순환 층
    10.4.3 양방향 RNN 사용하기
    10.4.4 더 나아가서

  10.5 요약


========================================
 11장 텍스트를 위한 딥러닝
========================================

  11.1 자연어 처리 소개

    - ELIZA     : https://ko.wikipedia.org/wiki/ELIZA
    - 튜링 테스트 : https://ko.wikipedia.org/wiki/튜링_테스트

  11.2 텍스트 데이터 준비

    11.2.1 텍스트 표준화
    11.2.2 텍스트 분할(토큰화)

    - N-gram에서 gram이란? : https://www.merriam-webster.com/dictionary/gram

    11.2.3 어휘 사전 인덱싱
    11.2.4 TextVectorization 층 사용하기

    - TextVectorization : https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt

  11.3 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스

    11.3.1 IMDB 영화 리뷰 데이터 준비하기
    11.3.2 단어를 집합으로 처리하기: BoW 방식
    11.3.3 단어를 시퀀스로 처리하기: 시퀀스 모델 방식

  11.4 트랜스포머 아키텍처

    11.4.1 셀프 어텐션 이해하기
    11.4.2 멀티 헤드 어텐션
    11.4.3 트랜스포머 인코더
    11.4.4 BoW 모델 대신 언제 시퀀스 모델을 사용하나요?

  11.5 텍스트 분류를 넘어: 시퀀스-투-시퀀스 학습

    11.5.1 기계 번역 예제
    11.5.2 RNN을 사용한 시퀀스-투-시퀀스 모델
    11.5.3 트랜스포머를 사용한 시퀀스-투-시퀀스 모델

  11.6 요약


========================================
  12장 생성 모델을 위한 딥러닝
========================================

  12.1 텍스트 생성

    12.1.1 시퀀스 생성을 위한 딥러닝 모델의 간단한 역사
    12.1.2 시퀀스 데이터를 어떻게 생성할까?
    12.1.3 샘플링 전략의 중요성
    12.1.4 케라스를 사용한 텍스트 생성 모델 구현
    12.1.5 가변 온도 샘플링을 사용한 텍스트 생성 콜백
    12.1.6 정리

  12.2 딥드림

    12.2.1 케라스 딥드림 구현
    12.2.2 정리

  12.3 뉴럴 스타일 트랜스퍼

    12.3.1 콘텐츠 손실
    12.3.2 스타일 손실
    12.3.3 케라스로 뉴럴 스타일 트랜스퍼 구현하기
    12.3.4 정리

  12.4 변이형 오토인코더를 사용한 이미지 생성

    12.4.1 이미지의 잠재 공간에서 샘플링하기
    12.4.2 이미지 변형을 위한 개념 벡터
    12.4.3 변이형 오토인코더
    12.4.4 케라스로 VAE 구현하기
    12.4.5 정리

  12.5 생성적 적대 신경망 소개

    12.5.1 GAN 구현 방법
    12.5.2 훈련 방법
    12.5.3 CelebA 데이터셋 준비하기
    12.5.4 판별자
    12.5.5 생성자
    12.5.6 적대 네트워크
    12.5.7 정리

  12.6 요약


===================================================
 13장 실전 문제 해결을 위한 모범 사례
===================================================

  13.1 모델의 최대 성능을 끌어내기

    13.1.1 하이퍼파라미터 최적화
    13.1.2 모델 앙상블

  13.2 대규모 모델 훈련하기

    13.2.1 혼합 정밀도로 GPU에서 훈련 속도 높이기
    13.2.2 다중 GPU 훈련
    13.2.3 TPU 훈련

  13.3 요약


=====================================
 14장 결론(Conclusions)
=====================================

  14.1 핵심 개념 리뷰(Key concepts in review)

    14.1.1 AI를 위한 여러 방법(Various approaches to AI)
    14.1.2 머신 러닝 분야에서 딥러닝이 특별한 이유(What makes deep learning special within the field of machine learning)
    14.1.3 딥러닝에 대해(How to think about deep learning)

    - 리차드 파인만 : https://namu.wiki/w/리처드%20파인만
    - 연결주의    : http://www.aistudy.com/neural/connectionism.htm

    14.1.4 핵심 기술(Key enabling technologies)

    - 클라우드도 핵심 기술이라 할 수 있을 듯

    14.1.5 일반적인 머신 러닝 워크플로(The universal machine learning workflow)
    14.1.6 주요 네트워크 구조(Key network architectures)

    - SeparableConv2D : https://ejleep1.tistory.com/1204

    14.1.7 딥러닝의 가능성(The space of possibilities)

  14.2 딥러닝의 한계(The limitations of deep learning)

    14.2.1 머신 러닝 모델의 의인화 위험(The risk of anthropomorphizing machine learning models)
    14.2.2 자동 기계 vs 지능 에이전트(Automatons vs. intelligent agents)
    14.2.3 지역 일반화 vs 궁극 일반화(Local generalization vs. extreme generalization)
    14.2.4 지능의 목적(The purpose of intelligence)
    14.2.5 일반화의 스펙트럼(Climbing the spectrum of generalization)

  14.3 AI에서 일반화를 높이기 위한 방법(Setting the course toward greater generality in AI)

    14.3.1 올바른 목표 설정의 중요성: 지름길 규칙(On the importance of setting the right objective: The shortcut rule)
    14.3.2 새로운 목표(A new target)

  14.4 지능 구현: 누락된 구성 요소(Implementing intelligence: The missing ingredients)

    14.4.1 추상적 비유에 뛰어난 지능(Intelligence as sensitivity to abstract analogies)
    14.4.2 두 종류의 추상화(The two poles of abstraction)
    14.4.3 누락된 절반의 그림(The missing half of the picture)

  14.5 딥러닝의 미래(The future of deep learning)

    14.5.1 프로그램 같은 모델(Models as programs)
    14.5.2 딥러닝과 프로그램 합성을 혼합하기(Blending together deep learning and program synthesis)
    14.5.3 영구 학습과 모듈화된 서브루틴 재사용(Lifelong learning and modular subroutine reuse)
    14.5.4 장기 비전(The long-term vision)

  14.6 빠른 변화에 뒤처지지 않기(Staying up to date in a fast-moving field)

    14.6.1 캐글의 실전 문제로 연습하기(Practice on real-world problems using Kaggle)
    14.6.2 아카이브(arXiv)를 통해 최신 논문 읽기(Read about the latest developments on arXiv)
    14.6.3 케라스 생태계 탐험하기(Explore the Keras ecosystem)

  14.7 맺음말(Final words)






