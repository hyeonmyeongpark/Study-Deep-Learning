
* 본 서브노트는 도서 '밑바닥부터 시작하는 딥러닝'의 목차를 기준으로 작성하였습니다

######################################
#
# 밑바닥부터 시작하는 딥러닝 
#
######################################

 * https://github.com/WegraLee/deep-learning-from-scratch

========================
 0장 인공 신경망
========================

# 용어 정리

  - ANN(Artificial Neural Network)

    - Perceptron 
    - DNN -> Deep          NN ≒ Multilayer perceptron ≒ Fully connected layers ≒ Dense layer
    - CNN -> Convolutional NN
    - RNN -> Recurrent     NN
    - ...


# 신경망 이해 

  신경망(Neural Network)
  - https://en.wikipedia.org/wiki/Neural_network

  인공 신경망(Artificial Neural Network)
  - https://ko.wikipedia.org/wiki/인공_신경망

  인공신경망/AI에 대한 간략한 히스토리
  - https://blog.naver.com/samsjang/221142104977
  - https://excelsior-cjh.tistory.com/172

  퍼셉트론
  - https://ko.wikipedia.org/wiki/퍼셉트론

    -> https://ko.wikipedia.org/wiki/프랑크_로젠블랫

  [딥러닝논문리뷰] 과연 딥러닝은 전지전능한가?
  - https://steemit.com/kr-science/@man-in-the-moon/3zfyqt


# 다양한 도서

  - 밑바닥부터 시작하는 딥러닝 2        -> RNN을 다루고 있음
  - 밑바닥부터 시작하는 딥러닝 3        -> 파이썬으로 직접 구현하며 배우는 딥러닝 프레임워크
  - 모두의 딥러닝 개정                  -> https://thebook.io/080228/
  - 파이썬 날코딩으로 알고 짜는 딥러닝  -> http://www.yes24.com/Product/Goods/75732024
  - ...

  - 실체가 손에 잡히는 딥러닝
    https://www.popit.kr/deep-learning-1/
    https://www.popit.kr/deep-learning-2/
    https://www.popit.kr/실체가-손에-잡히는-딥러닝3-이것만은-꼭-알아두자/


========================
 1장 헬로 파이썬        
========================

  - https://koreanfoodie.me/110


========================
 2장 퍼셉트론
========================

  - https://koreanfoodie.me/112
  - https://velog.io/@dscwinterstudy/밑바닥부터-시작하는-딥러닝-2장-wgk5jxneq5

# 뉴런과 퍼셉트론 비교

  https://blog.naver.com/gseducation/222430162038


# Perceptron 예제

  * 인공신경망에서 바이어스(Bias)의 역할
    https://chanhyeonglee.tistory.com/5

def my_perceptron(x1, x2, w1, w2, theta):
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1

# AND
my_perceptron(0, 0, 0.5, 0.5, 0.7)
my_perceptron(0, 1, 0.5, 0.5, 0.7)
my_perceptron(1, 0, 0.5, 0.5, 0.7)
my_perceptron(1, 1, 0.5, 0.5, 0.7)

# OR
my_perceptron(0, 0, 0.5, 0.5, 0.0)
my_perceptron(1, 0, 0.5, 0.5, 0.0)
my_perceptron(0, 1, 0.5, 0.5, 0.0)
my_perceptron(1, 1, 0.5, 0.5, 0.0)

# NAND
my_perceptron(0, 0, -0.5, -0.5, -0.7)
my_perceptron(0, 1, -0.5, -0.5, -0.7)
my_perceptron(1, 0, -0.5, -0.5, -0.7)
my_perceptron(1, 1, -0.5, -0.5, -0.7)


# 다층 퍼셉트론(Multi-Layer Perceptron; MLP)
def XOR(x1, x2):
    s1 = my_perceptron(x1, x2, -0.5, -0.5, -0.7) # NAND
    s2 = my_perceptron(x1, x2, 0.5, 0.5, 0.0)    # OR
    y1 = my_perceptron(s1, s2, 0.5, 0.5, 0.7)    # AND
    return y1

XOR(0, 0)
XOR(1, 0)
XOR(0, 1)
XOR(1, 1)


========================
 3장 신경망             
========================

  - https://koreanfoodie.me/113
  - https://koreanfoodie.me/114
  - https://velog.io/@dscwinterstudy/chapter3-신경망-pbk5gat98o

# Activation Function(활성화 함수) 소개 및 비교

  https://blog.naver.com/wideeyed/221017173808

  -> 시그모이드 함수

     https://ko.wikipedia.org/wiki/시그모이드_함수


# 다차원 배열의 계산 -> 연립일차방정식을 행렬로 풀기

  - https://ko.wikipedia.org/wiki/연립_일차_방정식

  x1*1 + x2*2 = y1
  x1*3 + x2*4 = y2
  x1*5 + x2*6 = y3


  X               W          Y
  ------------------------------------------
  (x1, x2)  dot  1, 3, 5  =  (y1, y2 ,y3) 
                 2, 4, 6

  1 X 2          2 X 3       1 X 3
 

# 학습 결과를 사용한다??

  - 보고서
  - Application에 탑재

    -> PyQT 예시

       https://codetorial.net/tensorflow/save_load_model.html
       https://codetorial.net/pyqt5/examples/mnist_classifier.html


# 신경망 구현

  https://velog.io/@dscwinterstudy/chapter3-신경망-pbk5gat98o

  - 신경망이란?
  - 활성화 함수의 종류
  - 다차원 배열의 계산
  - 3층 신경망 구현하기
  - 출력층 설계하기

-------------------------
> 3층 신경망 구현 정리  <
-------------------------

import numpy as np

def sigmoid(x):
  return 1/(1+np.exp(-x))
  
def identity_function(x):
  return x
  
def init_network():
  network = {}
  network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) #2*3
  network['b1'] = np.array([0.1, 0.2, 0.3]) #1*3
  network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) #3*2
  network['b2'] = np.array([0.1, 0.2]) #1*2
  network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) #2*2
  network['b3'] = np.array([0.1, 0.2]) #1*2
  
  return network

def forward(network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']
  
  a1 = np.dot(x, W1) + b1
  z1 = sigmoid(a1)
  a2 = np.dot(z1, W2) + b2
  z2 = sigmoid(a2)
  a3 = np.dot(z2, W3) + b3
  y = identity_function(a3)
  
  return y


network = init_network()
x = np.array([1.0, 0.5]) #1*2
y = forward(network, x)
print(y)


# Softmax 함수

  - https://teddylee777.github.io/machine-learning/softmax-함수란
  - https://bskyvision.com/427


========================
 4장 신경망 학습
========================

  - https://koreanfoodie.me/117
  - https://koreanfoodie.me/119
  - https://velog.io/@dscwinterstudy/밑바닥부터-시작하는-딥러닝-4장 

# 주요 내용 사전 이해

  -> One-hot Encoding

     https://teddylee777.github.io/machine-learning/python-numpy로-one-hot-encoding-쉽게하기

     np.eye(3)
     np.eye(3)[0]
     np.eye(3)[2]

     np.eye(num)[target]

  -> 비용함수 : MSE vs CEE 

     - 왜 비용함수 혹은 손실함수라 부르는가? 경제학에서 사용하던 용어아닌가? ... 원하는 대로 안되는 것을 가리키는 용어를 무엇으로 정할까?

     - Loss Function(손실 함수), Cost Function(비용 함수), Objective Function(목적 함수)

       https://keepdev.tistory.com/48

     - Deviation, Error, Residual -> https://en.wikipedia.org/wiki/Errors_and_residuals
     - Artificial neural network  -> https://en.wikipedia.org/wiki/Artificial_neural_network에서 cost 검색해 볼 것
     - Deep Learning Basics       -> https://www.coursera.org/lecture/intro-practical-deep-learning/deep-learning-basics-ZzAwS 대본에서

       The cost is a metric of difference 
       between the actual output Y hat and the expected output Y.

     - MSE vs CEE 

       http://www.gisdeveloper.co.kr/?p=7631

  -> 미니배치(Mini Batch)학습

     train_size = 60000
     batch_size = 600

     batch_mask = np.random.choice(train_size, batch_size)

  -> (미니배치용) CEE 구현

     - 정답 레이블이 원-핫 인코드인 경우
     - 정답 레이블이 '2'나 '7' 등의 숫자 레이블인 경우


# 내용 설명

  - NN에서 학습이란?

    - 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 과정
    - 비용함수(신경망이 학습할 수 있도록 해주는 지표)의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표임
    - 비용함수의 값을 가급적 작게 만드는 기법으로 함수의 기울기를 활용하는 경사법을 많이 활용함

  - NN은 데이터 주도 학습임

    -> 사람이 생각한 알고리즘
    -> 사람이 생각한 특징 + 머신러닝
    -> 데이터로부터 특징까지 학습하는 신경망(딥러닝) -> 신경망은 모든 문제를 같은 맥락으로 풀수있다는 이점이 있음

  - 미분과 학습

    https://www.mathsisfun.com/calculus/index.html

    미분값은 x에 대한 f(x)의 순간변화량(기울기)입니다. 
    함수의 기울기는 미분으로 구할 수 있습니다. 
    
    변수가 여럿인 함수에 대한 미분을 편미분이라하며 편미분 또한 미분과 마찬가지로 
    특정 장소의 기울기를 의미합니다. 여러 변수의 편미분을 벡터로 정리한 것이 또한
    기울기입니다.

    경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동합니다. 그런 다음 이동한 
    곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복합니다.
    이것을 경사법(gradient method)이라고 한다.

    신경망 학습에서는 최적의 매개변수(가중치와 편향)를 탐색할 때 비용 함수의 값을 
    가능한 한 작게 하는 매개변수 값을 찾습니다. 이때 매개변수의 미분(정확히는 기울기)을 
    계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복합니다.

    "가중치 매개변수의 비용함수의 미분"이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 
    비용함수가 어떻게 변하나'라는 의미입니다. 만약 이 미분 값이 음수면 그 가중치 매개변수를
    양의 방향으로 변화시켜 비용함수의 값을 줄일 수 있습니다. 반대로 이 미분값이 양수면
    그 가중치 매개변수를 음의 방향으로 변화시켜 비용함수의 값을 줄일 수 있습니다. 그러나
    미분 값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 비용함수의 값은 달라지지 않습니다.
    그래서 그 가중치 매개변수의 갱신은 거기서 멈춥니다.


# scikit-learn 이해 

  - scikit-learn_서브노트.txt


# 사이킷런으로 다층 신경망 훈련하기

  https://machfam.com/16450

  -> How can I determine "loss function" for MLPClassifier in skilearn?

     https://stackoverflow.com/questions/53369866/how-can-i-determine-loss-function-for-mlpclassifier-in-skilearn


# MLPClassifier 이해와 활용

  - MLPClassifier 이해와 활용.txt


========================
 5장 오차역전파법       
========================

  - https://koreanfoodie.me/120
  - https://koreanfoodie.me/157
  - https://velog.io/@dscwinterstudy/밑바닥부터-시작하는-딥러닝-5장-n3k5xl3thn

# [모두의 딥러닝 3판] 오차 역전파에서 딥러닝으로 : https://thebook.io/080324/part03/ch09/

  1 딥러닝의 태동, 오차 역전파
  2 활성화 함수와 고급 경사 하강법
  3 속도와 정확도 문제를 해결하는 고급 경사 하강법


# Keras 이해와 활용

  - Keras 이해와 활용.txt
  - 모두의 딥러닝 3판.txt


============================
 6장 학습 관련 기술들
============================

  - https://koreanfoodie.me/178
  - https://koreanfoodie.me/218
  - https://velog.io/@dscwinterstudy/Chapter6-학습-관련-기술들-가중치의-초깃값-a1k5xd5fsl
  

# 경사법

  - 딥러닝(Deep Learning) 살펴보기 1 : https://eoriented.github.io/post/deep-learning-overview-1/
  - 딥러닝(Deep Learning) 살펴보기 2 : https://eoriented.github.io/post/deep-learning-overview-2/

  - 다양한 경사하강법 가운데 최적은? : https://twinw.tistory.com/247
  - Adam을 개선한 RAdam              : https://theonly1.tistory.com/1754


============================
 7장 합성곱 신경망(CNN) 
============================

  - https://velog.io/@dscwinterstudy/2020-01-28-1501-작성됨-8zk5xihhpz


========================
 8장 딥러닝
========================

  - https://velog.io/@dscwinterstudy/2020-01-28-1401-작성됨-tfk5xgv65x 








                      









